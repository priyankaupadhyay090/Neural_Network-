{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Priyanka_Assignment_6.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50fujo0iZQp0"
      },
      "source": [
        "### Exercise 3 c) (3 points)\n",
        "\n",
        "This is intended to be a tutorial on PyTorch computation graphs. Your taks is to implement XOR. Each of the 3 subexercises is worth 1 point.\n",
        "\n",
        "The notebook provides code for you to start with. All necessary functions and classes are already imported. If you are unsure about how to use them, you can consult the corresponding documentation page (for example [torch.matmul](https://pytorch.org/docs/stable/generated/torch.matmul.html)). You may use numpy to define the XOR tables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq7GBNYZZQp6"
      },
      "source": [
        "# import necessary modules\n",
        "import torch\n",
        "import numpy as np # for data preparation\n",
        "from torch import matmul, sigmoid\n",
        "from torch.nn import MSELoss # Mean squared error\n",
        "\n",
        "torch.manual_seed(8)\n",
        "\n",
        "# number of epochs, learning rate and objective function should be the same for everyone\n",
        "epochs = 1000\n",
        "lr = 0.5\n",
        "criterion = MSELoss()\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyKf-4HbZQp7"
      },
      "source": [
        "## 1\n",
        "\n",
        "Now, as everything is imported, load the data into PyTorch tensors and define the model parameters. Explain the your decisions in 1-2 sentences each:\n",
        "\n",
        "a) How do you initialize the parameters (all zero, all ones, random distribution etc.)?\n",
        "\n",
        "What happens if biases are initialized as torch.ones(1)?\n",
        "\n",
        "b) Which of the variables need gradient tracking? What do you have to do to toggle it in PyTorch?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wARVho5rZQp8",
        "outputId": "d240ee2e-5660-4572-b2ce-6c3c67f0b23f"
      },
      "source": [
        "# data\n",
        "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32, requires_grad=True)\n",
        "Y = torch.tensor([0, 1, 1, 0], dtype=torch.float32, requires_grad=True).reshape(X.shape[0], 1)\n",
        "\n",
        "# params\n",
        "W_i = torch.rand(2, 2, requires_grad=True)\n",
        "W_o = torch.rand(2, 1, requires_grad=True)\n",
        "b_a = torch.zeros(1, 2, requires_grad=True)\n",
        "b_o = torch.zeros(1, 1, requires_grad=True)\n",
        "W_i\n",
        "\n",
        "\n",
        "## What happens if biases are initialized as torch.ones(1)? - our pricdted output will be mean toward bias term if we have no input, so to avoid this, we \n",
        "## inlialize bias with 0\n",
        "\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5979, 0.8453],\n",
              "        [0.9464, 0.2965]], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHf5BKRVZQp8"
      },
      "source": [
        "## 2\n",
        "\n",
        "The next step is to implement a full forward pass of the model. This means you have to implement the computation graph built in a) and present each XOR input once to it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftcsfHGZZQp8"
      },
      "source": [
        "# TODO: forward pass\r\n",
        "def forward_pass(x, wi, wo, ba, bo):\r\n",
        "    \r\n",
        "    hl_activation = torch.matmul(x, wi) + ba\r\n",
        "    hl_output = torch.sigmoid(hl_activation)\r\n",
        "    \r\n",
        "    output_activation = torch.matmul(hl_output, wo) + bo\r\n",
        "    predicted_output = torch.sigmoid(output_activation)\r\n",
        "    return (predicted_output,hl_activation)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk7ulZZrZQp8"
      },
      "source": [
        "## 3\n",
        "Enhance your model by actually doing backpropagation! \n",
        "\n",
        "Gradient from W_o, W_i + biases\n",
        "\n",
        "a) Employ the criterion defined in the first code cell to get a loss value and backpropagate by using the .backward() method. Then, access the gradient information in the tracked variables and perform the update. You can do so for a variable w by:\n",
        "\n",
        "```python\n",
        "w.data -= lr*w.grad.data\n",
        "```\n",
        "\n",
        "You also need to reset the gradient data after performing the update:\n",
        "\n",
        "```python\n",
        "w.grad.data.zero_()\n",
        "```\n",
        "\n",
        "You will have to loop over the data set several times to get a nice outcome. If your implementation is correct the values defined above (number of epochs, learning rate) will suffice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PDv8dHvZQp9"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    #TODO: Forward pass\n",
        "    prediction, _ = forward_pass(X, W_i, W_o, b_a, b_o)\n",
        "    #TODO: Backward pass\n",
        "    loss_fun = criterion(prediction, Y)\n",
        "    loss_fun.backward()\n",
        "    W_i.data -= lr*W_i.grad.data\n",
        "    W_o.data -= lr*W_o.grad.data\n",
        "    b_a.data -= lr*b_a.grad.data\n",
        "    b_o.data -= lr*b_o.grad.data\n",
        "    W_i.grad.data.zero_()\n",
        "    W_o.grad.data.zero_()\n",
        "    \n",
        "  "
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_biKmQ2ZQp9"
      },
      "source": [
        "b) Now, test your model on all data points! Did it learn?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b8M4xn6ZQp9",
        "outputId": "55dec3aa-7892-4161-a719-978e337a07ee"
      },
      "source": [
        "# TODO: Test\r\n",
        "\r\n",
        "X_test = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\r\n",
        "Output, hidd_act = forward_pass(X_test, W_i, W_o, b_a, b_o)\r\n",
        "test_prediction = (Output > 0.5) * 1.0\r\n",
        "print (test_prediction, Output)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.]]) tensor([[0.0943],\n",
            "        [0.8996],\n",
            "        [0.8996],\n",
            "        [0.1044]], grad_fn=<SigmoidBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf0jWzrwZQp-"
      },
      "source": [
        "c) Finally, print the final weight matrix W_o and the hidden activation for each item. Can you tell what the model does to separate the classes? Would it work without sigmoid activation? Explain your answer in 2-3 sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_ElPLmTZQp-",
        "outputId": "17a29a31-649f-42f1-bf82-230efec2e535"
      },
      "source": [
        "# TODO: print W_o & hidden activation\r\n",
        "\r\n",
        "print(W_i.data)\r\n",
        "print(W_o.data)\r\n",
        "print(b_a.data)\r\n",
        "print(b_o.data)\r\n",
        "print(hidd_act)\r\n",
        "print(Output)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2.9917, 0.9858],\n",
            "        [3.0104, 0.6011]])\n",
            "tensor([[ 2.6341],\n",
            "        [-0.6243]])\n",
            "tensor([[-0.3747,  0.0267]])\n",
            "tensor([[-1.6616]])\n",
            "tensor([[-0.3747,  0.0267],\n",
            "        [ 2.6358,  0.6278],\n",
            "        [ 2.6170,  1.0125],\n",
            "        [ 5.6275,  1.6136]], grad_fn=<AddBackward0>)\n",
            "tensor([[0.2881],\n",
            "        [0.5961],\n",
            "        [0.5830],\n",
            "        [0.6088]], grad_fn=<SigmoidBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8rCQNGmu91r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}